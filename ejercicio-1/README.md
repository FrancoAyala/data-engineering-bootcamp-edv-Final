# Ejercicio 1 â€“ Pipeline de AviaciÃ³n (Airflow + Spark + Hive)

Este ejercicio implementa un pipeline ETL completo para procesar datos de aviaciÃ³n argentina usando:

- **Apache Airflow** para orquestaciÃ³n
- **PySpark** para transformaciones distribuidas
- **Apache Hive** como data warehouse
- **HDFS** como sistema de archivos distribuido

---

## ğŸ“‚ Estructura del ejercicio

ejercicio-1/
â”‚â”€â”€ airflow/ # DAG de Airflow
â”‚â”€â”€ hive/ # DDL y queries de anÃ¡lisis
â”‚â”€â”€ scripts/ # Scripts de ingest y transformaciÃ³n
â”‚â”€â”€ images/ # ImÃ¡genes de DAG y consultas
â”‚â”€â”€ README.md # Este archivo




---

## ğŸš€ 1. DAG de Airflow

El DAG principal se encuentra en:
airflow/transform_aviacion_e2e.py


Este DAG realiza:

1. Descarga de datasets desde S3  
2. Ingesta a HDFS  
3. CreaciÃ³n de tablas Hive  
4. Limpieza y transformaciones  
5. InserciÃ³n de tablas finales

### ğŸ“¸ Imagen del DAG

<p align="center">
  <img src="./images/ejercicio1_DAG.png" width="800">
</p>



---

## ğŸ› ï¸ 2. Scripts utilizados

### Ingesta (bash)

scripts/ingest_aviacion.sh





### TransformaciÃ³n con PySpark

scripts/transformacion_aviacion.py



---

## ğŸ—„ï¸ 3. Tablas Hive

Se encuentran en:
hive/create_tables.sql





Incluye la creaciÃ³n del Data Warehouse y tablas staging/finales.

---

## ğŸ“Š 4. Consultas de anÃ¡lisis (KPIs)

Disponibles en:

hive/queries_aviacion.sql


### ğŸ“Š Resultados de las Queries
<p align="center">
  <img src="./images/ejercicio1_6.png" width="700">
</p>


<p align="center">
  <img src="./images/ejercicio1_7.png" width="700">
</p>

<p align="center">
  <img src="./images/ejercicio1_8.png" width="700">
</p>

<p align="center">
  <img src="./images/ejercicio1_9.png" width="700">
</p>


---

## ğŸ“ Notas

- Todas las rutas estÃ¡n parametrizadas en Variables de Airflow.
- Los CSV usan separador `;`.
- Se aplican transformaciones: casteos, limpieza, normalizaciÃ³n y uniones.

---

## âœ”ï¸ Estado

Pipeline funcional y validado con todas las consultas requeridas.






