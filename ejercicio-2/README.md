# ğŸš— Ejercicio 2 â€“ Pipeline de Car Rental (Airflow + Spark + Hive)

Este ejercicio implementa un pipeline ETL completo para procesar datos de alquileres de autos usando:

- Apache Airflow para orquestaciÃ³n  
- PySpark para transformaciones distribuidas  
- Apache Hive como data warehouse  
- HDFS como sistema de archivos distribuido  

---

## ğŸ“‚ Estructura del ejercicio

El directorio `ejercicio-2/` contiene:

- `airflow/` â†’ DAG Padre y DAG Hijo  
- `hive/` â†’ DDL y tabla externa  
- `scripts/` â†’ Scripts de ingesta y transformaciÃ³n  
- `images/` â†’ ImÃ¡genes del DAG e informes de consultas  
- `README.md` â†’ Este archivo  

---

## ğŸš€ 1. DAGs de Airflow

### ğŸ”· DAG Padre â€“ `car_rental_parent.py`

UbicaciÃ³n: `airflow/car_rental_parent.py`

Este DAG realiza:

1. Descarga de datasets desde S3  
2. Copia a la carpeta *landing*  
3. Ingesta a HDFS (`/car_rental/raw`)  
4. Trigger del DAG hijo (`car_rental_child`)  

#### ğŸ–¼ï¸ Imagen del DAG Padre

![DAG Padre](images/ejercicio2_dagF.png)

---

### ğŸ”¶ DAG Hijo â€“ `car_rental_child.py`

UbicaciÃ³n: `airflow/car_rental_child.py`

Este DAG se encarga de:

- Ejecutar el script de PySpark  
- Limpiar y normalizar columnas  
- Enriquecer con dataset de estados  
- Excluir registros de Texas (requisito del ejercicio)  
- Generar la capa *curated* en formato Parquet en HDFS  

#### ğŸ–¼ï¸ Imagen del DAG Hijo

![DAG Hijo](images/ejercicio2_dagC.png)

---

## ğŸ› ï¸ 2. Scripts utilizados

### ğŸ“Œ Ingesta (bash)

UbicaciÃ³n: `scripts/ingest_car_rental.sh`

Responsabilidades principales:

- Descargar `CarRentalData.csv` y `us_states.csv` desde S3  
- Crear carpetas de *landing* y RAW  
- Subir los archivos a `hdfs:///car_rental/raw`  
- Listar el contenido final en HDFS para validaciÃ³n  

---

### ğŸ“Œ TransformaciÃ³n con PySpark

UbicaciÃ³n: `scripts/transformation_car_rental.py`

Transformaciones aplicadas:

- NormalizaciÃ³n de nombres de columnas  
- `trim` y `lower` sobre cadenas de texto  
- ConversiÃ³n de tipos a `INT` donde corresponde  
- Join con el dataset de estados de EE.UU.  
- ExclusiÃ³n de registros de Texas  
- Escritura del dataset final en Parquet en:  
  `hdfs:///car_rental/curated/analytics/`

---

## ğŸ—„ï¸ 3. Tabla Hive

UbicaciÃ³n del DDL: `hive/create_table_car_rental.sql`

Se crea la base y tabla externa:

- Base: `car_rental_db`  
- Tabla: `car_rental_analytics` (EXTERNAL, PARQUET)  
- UbicaciÃ³n: `hdfs:///car_rental/curated/analytics/`  

---

## ğŸ“Š 4. Consultas de anÃ¡lisis (KPIs)

Las consultas de negocio se encuentran en:  
`hive/queries_car_rental.sql`

A continuaciÃ³n se muestran los resultados de cada punto del ejercicio:

### a) Cantidad de alquileres de autos ecolÃ³gicos (fuelType hÃ­brido o elÃ©ctrico, rating â‰¥ 4)

![Consulta A](images/ejercicio2_a.png)

---

### b) Los 5 estados con menor cantidad de alquileres

![Consulta B](images/ejercicio2_b.png)

---

### c) Los 10 modelos (y marcas) de autos mÃ¡s rentados

![Consulta C](images/ejercicio2_c.png)

---

### d) Cantidad de alquileres por aÃ±o, para autos fabricados entre 2010 y 2015

![Consulta D](images/ejercicio2_d.png)

---

### e) Las 5 ciudades con mÃ¡s alquileres de vehÃ­culos ecolÃ³gicos

![Consulta E](images/ejercicio2_e.png)

---

### f) Promedio de reviews segmentado por tipo de combustible

![Consulta F](images/ejercicio2_f.png)

---

## ğŸ“ Notas

- La ingesta usa URLs pÃºblicas de S3.  
- Los datos RAW se almacenan en `/car_rental/raw`.  
- La capa *curated* se genera en Parquet, optimizada para consulta.  
- Hive lee directamente desde la capa curated sin necesidad de mover datos.  

