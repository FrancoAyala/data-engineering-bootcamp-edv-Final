ğŸš— Ejercicio 2 â€“ Pipeline Car Rental (Airflow + Spark + Hive)
Este ejercicio implementa un pipeline ETL completo para procesar datos de alquileres de autos usando:

Apache Airflow para orquestaciÃ³n

PySpark para transformaciones distribuidas

Apache Hive como data warehouse

HDFS como sistema de archivos distribuido


ğŸ“‚ Estructura del ejercicio
ejercicio-2/ contiene:

airflow/ â†’ DAG Padre y DAG Hijo

hive/ â†’ Tabla externa y DDL

scripts/ â†’ Ingesta + transformaciÃ³n PySpark

images/ â†’ ImÃ¡genes del DAG y consultas

README.md â†’ Este archivo



ğŸš€ 1. DAGs de Airflow
ğŸ”· DAG Padre: car_rental_parent.py
Realiza:

Descarga datasets desde S3

Copia a landing

Ingesta a HDFS

Trigger del DAG hijo

ğŸ“¸ Imagen del DAG Padre
![DAG Padre](images/ejercicio2_dagF.png)


ğŸ”¶ DAG Hijo: car_rental_child.py
Realiza:

TransformaciÃ³n completa con PySpark

Escritura de datos curated en Parquet

PreparaciÃ³n final para Hive

ğŸ“¸ Imagen del DAG Hijo
![DAG Hijo](images/ejercicio2_dagC.png)




ğŸ› ï¸ 2. Scripts utilizados
ğŸ“Œ Ingesta (bash)
scripts/ingest_car_rental.sh

Funciones:

Descarga datasets desde S3

Limpieza de landing

CreaciÃ³n de carpetas

EnvÃ­o a HDFS (/car_rental/raw)

Listado final




ğŸ“Œ TransformaciÃ³n en PySpark
scripts/transformation_car_rental.py

Incluye:

Limpieza de columnas

ConversiÃ³n de tipos

NormalizaciÃ³n (trim, lower)

Join con tabla de estados

RemociÃ³n de datos errÃ³neos (ej: Texas)

Escritura en Parquet (/car_rental/curated/analytics/




ğŸ—ƒï¸ 3. Tabla Hive
Archivo: hive/create_table.sql

Crea la tabla externa final:
car_rental_analytics
ğŸ“¸ Evidencia de creaciÃ³n:
![Hive A](images/ejercicio2_a.png)
![Hive B](images/ejercicio2_b.png)



ğŸ“Š 4. Consultas de anÃ¡lisis (KPIs)
Archivo SQL: hive/queries_car_rental.sql

ğŸ“¸ Resultados de consultas:
![Consulta A](images/ejercicio2_a.png)
![Consulta B](images/ejercicio2_b.png)
![Consulta C](images/ejercicio2_c.png)
![Consulta D](images/ejercicio2_d.png)
![Consulta E](images/ejercicio2_e.png)
![Consulta F](images/ejercicio2_f.png)




ğŸ“ Notas
La ingesta usa descargables del repositorio pÃºblico (S3).

Los datos RAW se guardan en /car_rental/raw.

PySpark genera Parquet limpio preparado para anÃ¡lisis.

Hive consulta directamente el curated layer.
























